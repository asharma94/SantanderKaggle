{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing duplicate columns\n",
      "Dropping ID as feature\n",
      "Replacing missing var3 country value with mean\n",
      "Adding a feature for the sum of zeros\n",
      "Log transforming var38 and splitting var38 into two features\n",
      "Removing features with variance less than 1.000\n",
      "('the features removed were', ['ind_var1_0', 'ind_var1', 'ind_var2_0', 'ind_var5_0', 'ind_var5', 'ind_var6_0', 'ind_var6', 'ind_var8_0', 'ind_var8', 'ind_var12_0', 'ind_var12', 'ind_var13_0', 'ind_var13_corto_0', 'ind_var13_corto', 'ind_var13_largo_0', 'ind_var13_largo', 'ind_var13_medio_0', 'ind_var13', 'ind_var14_0', 'ind_var14', 'ind_var17_0', 'ind_var17', 'ind_var18_0', 'ind_var19', 'ind_var20_0', 'ind_var20', 'ind_var24_0', 'ind_var24', 'ind_var25_cte', 'ind_var26_0', 'ind_var26_cte', 'ind_var25_0', 'ind_var30_0', 'ind_var30', 'ind_var31_0', 'ind_var31', 'ind_var32_cte', 'ind_var32_0', 'ind_var33_0', 'ind_var33', 'ind_var34_0', 'ind_var37_cte', 'ind_var37_0', 'ind_var39_0', 'ind_var40_0', 'ind_var40', 'ind_var41_0', 'ind_var44_0', 'ind_var44', 'num_var1_0', 'num_var1', 'num_var4', 'num_var5_0', 'num_var6_0', 'num_var6', 'num_var8_0', 'num_var8', 'num_var12_0', 'num_var12', 'num_var13_0', 'num_var13_corto_0', 'num_var13_corto', 'num_var13_largo_0', 'num_var13_largo', 'num_var13_medio_0', 'num_var13', 'num_var14_0', 'num_var14', 'num_var17_0', 'num_var17', 'num_var18_0', 'num_var20_0', 'num_var20', 'num_var24_0', 'num_var24', 'num_var26_0', 'num_var25_0', 'num_op_var40_hace2', 'num_op_var40_hace3', 'num_var31_0', 'num_var31', 'num_var32_0', 'num_var33_0', 'num_var33', 'num_var34_0', 'num_var40_0', 'num_var40', 'num_var42_0', 'num_var44_0', 'num_var44', 'ind_var7_emit_ult1', 'ind_var7_recib_ult1', 'ind_var10_ult1', 'ind_var10cte_ult1', 'ind_var9_cte_ult1', 'ind_var9_ult1', 'ind_var43_emit_ult1', 'ind_var43_recib_ult1', 'num_aport_var13_hace3', 'num_aport_var13_ult1', 'num_aport_var17_hace3', 'num_aport_var17_ult1', 'num_aport_var33_hace3', 'num_aport_var33_ult1', 'num_var7_emit_ult1', 'num_var7_recib_ult1', 'num_compra_var44_hace3', 'num_compra_var44_ult1', 'num_ent_var16_ult1', 'num_meses_var8_ult3', 'num_meses_var12_ult3', 'num_meses_var13_corto_ult3', 'num_meses_var13_largo_ult3', 'num_meses_var13_medio_ult3', 'num_meses_var17_ult3', 'num_meses_var29_ult3', 'num_meses_var33_ult3', 'num_meses_var39_vig_ult3', 'num_meses_var44_ult3', 'num_op_var40_efect_ult1', 'num_op_var40_efect_ult3', 'num_reemb_var13_ult1', 'num_reemb_var17_hace3', 'num_reemb_var17_ult1', 'num_reemb_var33_ult1', 'num_sal_var16_ult1', 'num_trasp_var17_in_hace3', 'num_trasp_var17_in_ult1', 'num_trasp_var17_out_ult1', 'num_trasp_var33_in_hace3', 'num_trasp_var33_in_ult1', 'num_trasp_var33_out_ult1', 'num_venta_var44_hace3', 'num_venta_var44_ult1', 'saldo_medio_var29_hace3'])\n",
      "Adding top 5 principal components\n",
      "One hot encoding countries\n",
      "(33,)\n",
      "(33,)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#note: all functions modify input frame\n",
    "\n",
    "#this one might not actually be that useful \n",
    "def drop_duplicate_cols(training, test) :\n",
    "    print(\"Removing duplicate columns\")\n",
    "    remove = []\n",
    "    cols = training.columns\n",
    "    for i in range(len(cols)-1):\n",
    "        v = training[cols[i]].values\n",
    "        for j in range(i+1,len(cols)):\n",
    "            if np.array_equal(v,training[cols[j]].values):\n",
    "                remove.append(cols[j])\n",
    "    training = training.drop(remove, axis=1)\n",
    "    test = test.drop(remove, axis=1)\n",
    "    return training, test\n",
    "\n",
    "\n",
    "def replace_var3_with_mean(training, test):\n",
    "    print(\"Replacing missing var3 country value with mean\")\n",
    "    training = training.replace(-999999,2)\n",
    "    test = test.replace(-999999,2)\n",
    "    return training, test\n",
    "\n",
    "def one_hot_encode_countries(training, test):\n",
    "    print(\"One hot encoding countries\")\n",
    "    train = pd.get_dummies(training, columns = ['var3'] )\n",
    "    test = pd.get_dummies(test, columns = ['var3'])\n",
    "\n",
    "    # get the columns in train that are not in test\n",
    "    col_to_add = np.setdiff1d(train.columns, test.columns)\n",
    "    print col_to_add.shape\n",
    "    col_to_add2 = np.setdiff1d(test.columns, train.columns)\n",
    "    print col_to_add.shape\n",
    "    # add these columns to test, setting them equal to zero\n",
    "    for c in col_to_add:\n",
    "        test[c] = 0\n",
    "\n",
    "    # select and reorder the test columns using the train columns\n",
    "    test = test[train.columns]\n",
    "    return train, test\n",
    "    \n",
    "def add_feature_for_sum_of_zeros(training, test):\n",
    "    print(\"Adding a feature for the sum of zeros\")\n",
    "    X = training.iloc[:,:-1]\n",
    "    Xt = test.iloc[:,:-1]\n",
    "    X['n0'] = (X==0).sum(axis=1)\n",
    "    Xt['n0'] = (Xt==0).sum(axis=1)\n",
    "    training['n0'] = X['n0']\n",
    "    test['n0'] = Xt['n0']\n",
    "    return training, test\n",
    "    \n",
    "\n",
    "def log_transform_var38_and_split_into_two_features(training, test):\n",
    "    print(\"Log transforming var38 and splitting var38 into two features\")\n",
    "    training['var38ismode'] = np.isclose(training.var38, 117310.979016)\n",
    "    training['logvar38'] = training.loc[~training['var38ismode'], 'var38'].map(np.log)\n",
    "    training.loc[training['var38ismode'], 'logvar38'] = 0\n",
    "\n",
    "    test['var38ismode'] = np.isclose(test.var38, 117310.979016)\n",
    "    test['logvar38'] = test.loc[~test['var38ismode'], 'var38'].map(np.log)\n",
    "    test.loc[test['var38ismode'], 'logvar38'] = 0\n",
    "\n",
    "    return training, test\n",
    "    \n",
    "def add_top_5_principal_components(training, test):\n",
    "    print(\"Adding top 5 principal components\")\n",
    "    pca = PCA(n_components=5)\n",
    "    training_copy = training.drop(['TARGET'], axis=1)\n",
    "    features = training_copy.columns\n",
    "    pca_training = pca.fit_transform(normalize(training[features], axis=0))\n",
    "    pca_test = pca.transform(normalize(test[features], axis=0))\n",
    "    training['PCA_0'] = pca_training[:,0]\n",
    "    training['PCA_1'] = pca_training[:,1]\n",
    "    training['PCA_2'] = pca_training[:,2]\n",
    "    training['PCA_3'] = pca_training[:,3]\n",
    "    training['PCA_4'] = pca_training[:,4]\n",
    "    test['PCA_0'] = pca_test[:,0]\n",
    "    test['PCA_1'] = pca_test[:,1]\n",
    "    test['PCA_2'] = pca_test[:,2]\n",
    "    test['PCA_3'] = pca_test[:,3]\n",
    "    test['PCA_4'] = pca_test[:,4]\n",
    "    \n",
    "    return training, test\n",
    "\n",
    "def remove_low_variance_features(training, test, threshold):\n",
    "    print(\"Removing features with variance less than %.3f\" % threshold)\n",
    "    remove = []\n",
    "    for col in training.columns:\n",
    "        if training[col].std() <= threshold:\n",
    "            remove.append(col)\n",
    "    dontremove = ['TARGET', 'var38ismode', 'var3']\n",
    "    for elem in dontremove:\n",
    "        if elem in remove: remove.remove(elem)\n",
    "            \n",
    "    #print remove\n",
    "    print(\"the features removed were\", remove)\n",
    "    training = training.drop(remove, axis=1)\n",
    "    test = test.drop(remove, axis=1)\n",
    "    return training, test\n",
    "\n",
    "def plot_low_variance_features(training, test):\n",
    "    stds = []\n",
    "    feature = []\n",
    "    for col in training.columns:\n",
    "        feature.append(col)\n",
    "        stds.append(training[col].std() ** 2)\n",
    "    \n",
    "    plt.scatter(stds, [0] * len(stds))\n",
    "    \n",
    "def drop_ID(training, test):\n",
    "    print(\"Dropping ID as feature\")\n",
    "    training = training.drop(['ID'], axis=1)\n",
    "    test = test.drop(['ID'], axis=1)\n",
    "    return training, test\n",
    "\n",
    "def standardize_data(training, test):\n",
    "    print(\"Standardizing features\")\n",
    "    features = []\n",
    "    for col in training.columns:\n",
    "        features.append(col)\n",
    "        \n",
    "    dontremove = ['TARGET', 'var38ismode', 'var3']\n",
    "    for elem in dontremove:\n",
    "        if elem in features: features.remove(elem)\n",
    "    \n",
    "    ss = StandardScaler()\n",
    "    training[features] = np.round(ss.fit_transform(training[features]), 6)\n",
    "    test[features] = np.round(ss.transform(test[features]), 6)\n",
    "    return training, test\n",
    "\n",
    "def add_k_means_cluster_as_feature(training, test):\n",
    "    return training, test\n",
    "    \n",
    "\n",
    "def main(standardize=False, threshold = 1.0, dropID=True):\n",
    "    training = pd.read_csv('train.csv')\n",
    "    test = pd.read_csv('test.csv')\n",
    "    training, test = drop_duplicate_cols(training, test)\n",
    "    if dropID:\n",
    "        training, test = drop_ID(training, test)\n",
    "    training, test = replace_var3_with_mean(training, test)\n",
    "    training, test = add_feature_for_sum_of_zeros(training, test)\n",
    "    training, test = log_transform_var38_and_split_into_two_features(training, test)\n",
    "    training, test = remove_low_variance_features(training, test, threshold)\n",
    "    training, test = add_top_5_principal_components(training, test)\n",
    "    \n",
    "    if standardize:\n",
    "        training, test = standardize_data(training, test)\n",
    "    training, test = one_hot_encode_countries(training, test)\n",
    "    return training, test\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    training, test = main(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
