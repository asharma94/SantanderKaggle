{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import *\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.utils import np_utils\n",
    "from keras import optimizers\n",
    "import keras\n",
    "\n",
    "import preprocessing\n",
    "\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing duplicate columns\n",
      "Replacing missing var3 country value with mean\n",
      "Adding a feature for the sum of zeros\n",
      "Log transforming var38 and splitting var38 into two features\n",
      "Removing features with variance less than 0.500\n",
      "the features removed were ['ind_var1_0', 'ind_var1', 'ind_var2_0', 'ind_var5_0', 'ind_var5', 'ind_var6_0', 'ind_var6', 'ind_var8_0', 'ind_var8', 'ind_var12_0', 'ind_var12', 'ind_var13_0', 'ind_var13_corto_0', 'ind_var13_corto', 'ind_var13_largo_0', 'ind_var13_largo', 'ind_var13_medio_0', 'ind_var13', 'ind_var14_0', 'ind_var14', 'ind_var17_0', 'ind_var17', 'ind_var18_0', 'ind_var19', 'ind_var20_0', 'ind_var20', 'ind_var24_0', 'ind_var24', 'ind_var25_cte', 'ind_var26_0', 'ind_var26_cte', 'ind_var25_0', 'ind_var30_0', 'ind_var30', 'ind_var31_0', 'ind_var31', 'ind_var32_cte', 'ind_var32_0', 'ind_var33_0', 'ind_var33', 'ind_var34_0', 'ind_var37_cte', 'ind_var37_0', 'ind_var39_0', 'ind_var40_0', 'ind_var40', 'ind_var41_0', 'ind_var44_0', 'ind_var44', 'num_var1_0', 'num_var1', 'num_var6_0', 'num_var6', 'num_var13_largo_0', 'num_var13_largo', 'num_var13_medio_0', 'num_var14', 'num_var17_0', 'num_var17', 'num_var18_0', 'num_var20_0', 'num_var20', 'num_op_var40_hace3', 'num_var31_0', 'num_var31', 'num_var32_0', 'num_var33_0', 'num_var33', 'num_var34_0', 'num_var40_0', 'num_var40', 'num_var44_0', 'num_var44', 'ind_var7_emit_ult1', 'ind_var7_recib_ult1', 'ind_var10_ult1', 'ind_var10cte_ult1', 'ind_var9_cte_ult1', 'ind_var9_ult1', 'ind_var43_emit_ult1', 'ind_var43_recib_ult1', 'num_aport_var13_ult1', 'num_aport_var17_hace3', 'num_aport_var17_ult1', 'num_aport_var33_hace3', 'num_aport_var33_ult1', 'num_var7_emit_ult1', 'num_var7_recib_ult1', 'num_compra_var44_hace3', 'num_compra_var44_ult1', 'num_meses_var8_ult3', 'num_meses_var12_ult3', 'num_meses_var13_corto_ult3', 'num_meses_var13_largo_ult3', 'num_meses_var13_medio_ult3', 'num_meses_var17_ult3', 'num_meses_var29_ult3', 'num_meses_var33_ult3', 'num_meses_var44_ult3', 'num_op_var40_efect_ult1', 'num_op_var40_efect_ult3', 'num_reemb_var13_ult1', 'num_reemb_var17_hace3', 'num_reemb_var17_ult1', 'num_reemb_var33_ult1', 'num_sal_var16_ult1', 'num_trasp_var17_in_hace3', 'num_trasp_var17_in_ult1', 'num_trasp_var17_out_ult1', 'num_trasp_var33_in_hace3', 'num_trasp_var33_in_ult1', 'num_trasp_var33_out_ult1', 'num_venta_var44_hace3', 'num_venta_var44_ult1']\n",
      "Adding top 5 principal components\n",
      "Standardizing features\n",
      "One hot encoding countries\n"
     ]
    }
   ],
   "source": [
    "train_set, test_set = preprocessing.main(dropID=False, threshold=0.5, standardize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('train.csv')\n",
    "test_set = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = train_set.drop('TARGET', axis=1)\n",
    "y_train = train_set['TARGET']\n",
    "\n",
    "X_test = test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(\n",
    "    # Tree params\n",
    "    gamma=0,\n",
    "    max_depth=5,\n",
    "    min_child_weight=9,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=1,\n",
    "    \n",
    "    objective= 'binary:logistic',\n",
    "    seed=5\n",
    ")\n",
    "\n",
    "def xgbParamSearch(X_train, y_train, X_test):\n",
    "    cv_params = {'max_depth': [3,4], 'learning_rate': [0.1], 'min_child_weight' : [5, 7], \n",
    "                 'subsample' : [0.8, 0.9], 'colsample_bytree' : [0.8, 0.9]}\n",
    "    GBM = GridSearchCV(model, cv_params, scoring = 'accuracy', cv = 2, verbose=True)\n",
    "    GBM.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = GBM.predict_proba(X_test)[:,1]\n",
    "    y_pred_train = GBM.predict_proba(X_train)[:,1]\n",
    "    \n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_train, y_pred_train)\n",
    "    print('xgb: %d', metrics.auc(fpr, tpr))\n",
    "    \n",
    "    model.set_params(max_depth=GBM.best_params_['max_depth'])\n",
    "    model.set_params(learning_rate=GBM.best_params_['learning_rate'])\n",
    "    model.set_params(max_depth=GBM.best_params_['min_child_weight'])\n",
    "    model.set_params(learning_rate=GBM.best_params_['subsample'])\n",
    "    model.set_params(learning_rate=GBM.best_params_['learning_rate'])\n",
    "    \n",
    "    return y_pred\n",
    "    \n",
    "\n",
    "def xgbEarlyStopping(X_train, y_train, X_test):\n",
    "    xgb_param = model.get_xgb_params()\n",
    "    dtrain = xgb.DMatrix(X_train.values, label=y_train.values, missing=np.nan)\n",
    "    cv_result = xgb.cv(\n",
    "        xgb_param,\n",
    "        dtrain,\n",
    "        num_boost_round=model.get_params()['n_estimators'],\n",
    "        nfold=5,\n",
    "        metrics=['auc'],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=10)\n",
    "    best_n_estimators = cv_result.shape[0]\n",
    "    model.set_params(n_estimators=best_n_estimators)\n",
    "\n",
    "    model.fit(X_train, y_train, eval_metric='auc')\n",
    "    feat_imp = pd.Series(model.booster().get_fscore()).sort_values(ascending=False)\n",
    "    index = pd.Series(model.booster().get_fscore()).sort_values(ascending=False).index\n",
    "    print((index, feat_imp))\n",
    "    \n",
    "    y_pred = model.predict_proba(X_test)[:,1]\n",
    "    y_pred_train = model.predict_proba(X_train)[:,1]\n",
    "    \n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_train, y_pred_train)\n",
    "    print('xgb: %d', metrics.auc(fpr, tpr))\n",
    "    \n",
    "    features = pd.Series(model.booster().get_fscore()).sort_values(ascending=False)\n",
    "    index = pd.Series(model.booster().get_fscore()).sort_values(ascending=False).index\n",
    "    \n",
    "    '''\n",
    "    pos = np.arange(len(index[:15]))\n",
    "    width = 0.8     # gives histogram aspect to the bar diagram\n",
    "\n",
    "    ax = plt.axes()\n",
    "    ax.set_yticks(pos + (width / 2))\n",
    "    ax.set_yticklabels(index[:15])\n",
    "\n",
    "    ax.set_title(\"Distribution of F-scores\")\n",
    "    ax.set_xlabel(\"relative importance\")\n",
    "\n",
    "    plt.barh(pos[:15], features[:15], width, color='royalblue')\n",
    "    plt.show()\n",
    "    '''\n",
    "\n",
    "    submission = pd.DataFrame({\"index\":index, \"features\":features})\n",
    "    submission.to_csv(\"scores.csv\", index=False)\n",
    "    \n",
    "    return y_pred\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "def keras_NN(Xtrain, ytrain, Xtest):\n",
    "    \n",
    "    Xtrain = StandardScaler().fit_transform(Xtrain)\n",
    "    Xtest = StandardScaler().fit_transform(Xtest)\n",
    "    \n",
    "    train_X, test_X, train_y, test_y = train_test_split(Xtrain, ytrain, train_size=0.9, random_state=0)\n",
    "    '''\n",
    "    models = Sequential()\n",
    "\n",
    "    models.add(Dense(120, input_shape=(Xtrain.shape[1],), init='uniform', W_regularizer=l2(0.00001)))\n",
    "    models.add(PReLU())\n",
    "    models.add(BatchNormalization(mode=2))\n",
    "    models.add(Dropout(0.6))\n",
    "    models.add(Dense(1, init='uniform'))\n",
    "    models.add(Activation('softmax'))\n",
    "\n",
    "    opt = optimizers.Adagrad(lr=0.0125)\n",
    "    models.compile(loss='binary_crossentropy', optimizer=opt) \n",
    "    models.fit(train_X, train_y, class_weight={0:0.0396, 1:0.9604})\n",
    "    models.evaluate(test_X, test_y)\n",
    "    \n",
    "    \n",
    "    y_pred = models.predict_proba(Xtest)\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_dim=Xtrain.shape[1],\n",
    "                    output_dim=400,\n",
    "                    init='uniform',\n",
    "                    activation='tanh'))\n",
    "\n",
    "    model.add(Dense(input_dim=400,\n",
    "                    output_dim=1,\n",
    "                    init='uniform',\n",
    "                    activation='sigmoid'))\n",
    "    \n",
    "    opt = optimizers.Adagrad(lr=0.0125)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "\n",
    "    model.fit(train_X, train_y, nb_epoch=10, class_weight={0:0.0396, 1:0.9604})\n",
    "    model.evaluate(test_X, test_y)\n",
    "\n",
    "    y_pred = model.predict_proba(Xtest)\n",
    "    y_pred = [item for sublist in y_pred for item in sublist]\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 16 candidates, totalling 32 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  32 out of  32 | elapsed:  3.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb: %d 0.854269686231\n",
      "[ 0.06043969  0.0559074   0.00210035 ...,  0.00370104  0.06048683\n",
      "  0.0014752 ]\n"
     ]
    }
   ],
   "source": [
    "y_pred= xgbParamSearch(X_train, y_train, X_test)\n",
    "#y_pred = xgbEarlyStopping(X_train, y_train, X_test)\n",
    "#y_pred = keras_NN(X_train.as_matrix(), y_train.as_matrix(), X_test.as_matrix())\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('test.csv')\n",
    "\n",
    "submission = pd.DataFrame({\"ID\":X_test['ID'], \"TARGET\":y_pred})\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from Sruthi's python files\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def randomforest(X_train, y_train, X_test):\n",
    "\n",
    "    rfc = RandomForestClassifier(n_jobs=-1,max_features= 'sqrt' ,n_estimators=400, oob_score = True)  \n",
    "    #grid = GridSearchCV(rfc, parameters, cv=5)\n",
    "    #grid.fit(X_train, y_train)\n",
    "    #print grid.best_params_\n",
    "    rfc.fit(X_train, y_train)\n",
    "    clf_probability = rfc.predict_proba(X_test)\n",
    "    #clf_probability = grid.predict_proba(X_test)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To use this, you need to return the model in the code above.\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(Xtrain, ytrain, train_size=0.9, random_state=0)\n",
    "\n",
    "xgb_train = xgbParamSearch(train_X, train_y, test X)\n",
    "xgbes_train = xgbEarlyStopping(train_X, train_y, test X)\n",
    "nn_train = keras_NN(train_X, train_y, test X)\n",
    "rf_train = randomforest(train_X, train_y, test X)\n",
    "\n",
    "blended_train_set = np.vstack((xgb_train, xgbes_train, rf_train, nn_train))\n",
    "blended_test_set = np.vstack((xgb_test, xgbes_test, rf_test, nn_train))\n",
    "\n",
    "blended_train_df = pd.DataFrame(blended_train_set.T, columns=['xgb-grid', 'xgb-es', 'rand-f', 'nn_train'])\n",
    "blended_test_df = pd.DataFrame(blended_test_set.T, columns=['xgb-grid', 'xgb-es', 'rand-f', 'nn_train'])\n",
    "\n",
    "y_pred_train, y_pred_test = logistic_regression(blended_test_df, test_y, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
